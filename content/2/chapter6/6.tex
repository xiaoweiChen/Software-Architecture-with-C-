

即使内存很多，了解如何使用它的也是个好主意。通常，内存吞吐量是现代系统的性能瓶颈，因此充分利用它很重要。执行太多的动态分配可能会降低程序的速度，并导致内存碎片。让我们来了解一些缓解这些问题的方法。

\subsubsubsection{6.6.1\hspace{0.2cm}使用SSO/SOO减少动态分配}

动态分配有时会给带来其他麻烦，而不仅仅是在没有足够内存的情况下构造对象时抛出问题。它们通常会占用CPU周期，并可能导致内存碎片。幸运的是，有一种方法可以避免这种情况。如果曾经使用过\texttt{std::string}(GCC 5.0之后)，那么可能使用的优化是一种名为小字符优化(Small string optimization，SSO)的策略。这是一个名为小对象优化(Small Object optimization, SSO)的更通用的优化示例，可以在\Abseil的\texttt{InlinedVector}类型中看到。主要思想很简单:如果动态分配的对象足够小，就应该存储在拥有它的类中，而不是动态分配。在\texttt{std::string}中，通常有容量、长度和存储的实际字符串。如果字符串足够短(对于GCC来说，在64位平台上是15字节)，将存储在其中的一些成员中。

就地存储对象，而不是进行分配，只存储指针还有一个好处:更少的指针跟踪。每次需要访问存储在指针后面的数据时，都会增加CPU缓存的压力，并面临从主存获取数据的风险。如果这是一个常见的模式，会影响应用程序的整体性能，特别是指向的地址没有被CPU的预取器猜到。使用SSO和SOO等技术对于减少这些问题十分有用。

\subsubsubsection{6.6.2\hspace{0.2cm}通过COW来节省内存}

如果您在GCC 5.0之前使用了GCC的\texttt{std::string}，那么可能使用了另一种名为Copy-On-Write(COW)的优化。当使用相同的底层字符数组创建多个实例时，COW字符串实现实际上共享相同的内存地址。将字符串写入时，将复制存储的数据——因此有了名称。

这种技术有助于节省内存并保持缓存热度，通常在单个线程上提供稳定的性能。但要注意不要在多线程上下文中使用它，使用锁可能是真正的性能杀手。与任何与性能相关的主题一样，最好只是衡量它是否匹配目前的工作。

现在来讨论C++17的一个特性，可以通过动态分配实现良好的性能。

\subsubsubsection{6.6.3\hspace{0.2cm}使用多态分配器}

这里讨论的特性是多态分配器。具体来说，是分配程序用来分配内存的\texttt{std::pmr::polymorphic\_allocator}和多态的\texttt{std::pmr::memory\_resource}类。

本质上，可以轻松地链接内存资源，以最大限度地利用内存。链的实现可以很简单，比如将资源保留一个大块并分发，然后回到另一个资源，如果它耗尽了，则只调用new和delete。实现也可以更复杂:可以构建一个处理不同大小的池的长内存资源链，只在需要时提供线程安全，绕过堆并直接获取系统内存，返回最后释放的内存块以提供缓存热度，以及做其他有趣的事情。所有这些功能都是由标准的多态内存资源提供的，因为它们的设计，想扩展他们就会很容易。

让我们先来讨论存储区。

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{存储区}

A memory arena, also called a region, is just a large chunk of memory that exists for a limited time. You can use it to allocate smaller objects that you use for the lifetime of the arena. Objects in the arena can be either deallocated as usual or erased all at once in a process called winking out. We'll describe it later on.

Arenas have several great advantages over the usual allocations and deallocations – they increase performance because they limit the memory allocations that need to grab upstream resources. They also reduce fragmentation of memory, because any fragmentation that would happen will happen inside the arena. Once an arena's memory is released, the fragmentation is no more as well. A great idea is to create separate arenas per thread. If only a single thread uses an arena, it doesn't need to use any locking or other thread-safety mechanisms, reducing thread contention and giving you a nice boost in performance.

If your program is single-threaded, a low-cost solution to increase its performance could be as follows:

\begin{lstlisting}[style=styleCXX]
auto single_threaded_pool = std::pmr::unsynchronized_pool_resource();
std::pmr::set_default_resource(&single_threaded_pool);
\end{lstlisting}

The default resource if you won't set any explicitly will be new\_delete\_resource, which calls new and delete each time just like regular std::allocator does, and with all the thread-safety it provides (and costs).

If you use the preceding code snippet, all the allocations done using pmr allocators would be done with no locks. You still need to actually use the pmr types, though. To do so with standard containers, for instance, you need to simply pass std::pmr::polymorphic\_allocator<T> as the allocator template parameter. Many standard containers have pmr-enabled type aliases. The two variables created next are of the same type and both will use the default memory resource:

\begin{lstlisting}[style=styleCXX]
auto ints = std::vector<int,
std::pmr::polymorphic_allocator<int>>(std::pmr::get_default_resource());
auto also_ints = std::pmr::vector<int>{};
\end{lstlisting}

The first one gets the resource passed explicitly, though. Let's now go through the resources available in pmr.

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{单调内存资源}

The first one we'll discuss is std::pmr::monotonic\_buffer\_resource. It's a resource that only allocates memory and doesn't do anything on deallocation. It will only deallocate memory when the resource is destructed or on an explicit call to release(). This, connected with no thread safety, makes this type extremely performant. If your application occasionally needs to perform a task that does lots of allocations on a given thread, then releases all the objects used at once afterward, using monotonic resources will yield great gains. It's also a great base building block for chains of resources.

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{资源池}

A common combo of two resources is to use a pool resource on top of a monotonic buffer resource. The standard pool resources create pools of different-sized chunks. There are two types in std::pmr, unsynchronized\_pool\_resource for use when only one thread allocates and deallocates from it and synchronized\_pool\_resource for multi-threaded use. Both should provide you with much better performance compared to the global allocator, especially when using the monotonic buffer as their upstream resource. If you wonder how to chain them, here's how:

\begin{lstlisting}[style=styleCXX]
auto buffer = std::array<std::byte, 1 * 1024 * 1024>{};
auto monotonic_resource =
std::pmr::monotonic_buffer_resource{buffer.data(), buffer.size()};
auto pool_options = std::pmr::pool_options{.max_blocks_per_chunk = 0,
	.largest_required_pool_block = 512};
auto arena =
std::pmr::unsynchronized_pool_resource{pool_options,
	&monotonic_resource};
\end{lstlisting}

We create a 1 MB buffer for the arena to reuse. We pass it to a monotonic resource, which is then passed to an unsynchronized pool resource, creating a simple yet efficient chain of allocators that won't call new until all the initial buffer is used up.

You can pass a std::pmr::pool\_options object to both the pool types to limit the max count of blocks of a given size (max\_blocks\_per\_chunk) or the size of the largest block (largest\_required\_pool\_block). Passing 0 causes the implementation's default to be used. In the case of GCC's library, the actual blocks per chunk differ depending on the block size. If the max size is exceeded, the pool resource will allocate directly from its upstream resource. It also goes to the upstream resource if the initial memory was depleted. In this case, it allocates geometrically growing chunks of memory.

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{编写自己的内存资源}

If the standard memory resources don't suit all your needs, you can always create a custom one quite simply. For instance, a good optimization that not all standard library implementations offer is to keep track of the last chunks of a given size that were released and return them back on the next allocations of given sizes. This Most Recently Used cache can help you increase the hotness of data caches, which should help your app's performance. You can think of it as a set of LIFO queues for chunks.

Sometimes you might also want to debug allocations and deallocations. In the following snippet, I have written a simple resource that can help you with this task:

\begin{lstlisting}[style=styleCXX]
class verbose_resource : public std::pmr::memory_resource {
	std::pmr::memory_resource *upstream_resource_;
public:
	explicit verbose_resource(std::pmr::memory_resource *upstream_resource)
		: upstream_resource_(upstream_resource) {}
\end{lstlisting}

Our verbose resource inherits from the polymorphic base resource. It also accepts an upstream resource, which it will use for actual allocations. It has to implement three private functions – one for allocating, one for deallocating, and one for comparing instances of the resource itself. Here's the first one:

\begin{lstlisting}[style=styleCXX]
private:
void *do_allocate(size_t bytes, size_t alignment) override {
	std::cout << "Allocating " << bytes << " bytes\n";
	return upstream_resource_->allocate(bytes, alignment);
}
\end{lstlisting}

All it does is print the allocation size on the standard output and then use the upstream resource to allocate memory. The next one will be similar:

\begin{lstlisting}[style=styleCXX]
	void do_deallocate(void *p, size_t bytes, size_t alignment) override {
		std::cout << "Deallocating " << bytes << " bytes\n";
		upstream_resource_->deallocate(p, bytes, alignment);
	}
\end{lstlisting}

We log how much memory we deallocate and use the upstream to perform the task. Now the last required function is stated next:

\begin{lstlisting}[style=styleCXX]
	[[nodiscard]] bool
	do_is_equal(const memory_resource &other) const noexcept override {
		return this == &other;
	}
\end{lstlisting}

We simply compare the addresses of the instances to know whether they're equal. The [[nodiscard]] attribute helps us be sure that the caller actually consumes the returned value, which can help us avoid accidental misuse of our function.

That's it. For a powerful feature such as the pmr allocators, the API isn't that complex now, isn't it?

Aside from tracking allocations, we can also use pmr to guard us against allocating when we shouldn't.

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{确保没有意外的分配}

The special std::pmr::null\_memory\_resource() will throw an exception when anyone tries to allocate memory using it. You can safeguard from performing any allocations using pmr by setting it as the default resource as shown next:

\begin{lstlisting}[style=styleCXX]
std::pmr::set_default_resource(null_memory_resource());
\end{lstlisting}

You can also use it to limit allocation from the upstream when it shouldn't happen. Check the following code:

\begin{lstlisting}[style=styleCXX]
auto buffer = std::array<std::byte, 640 * 1024>{}; // 640K ought to be
enough for anybody
auto resource = std::pmr::monotonic_buffer_resource{
	buffer.data(), buffer.size(), std::pmr::null_memory_resource()};
\end{lstlisting}

If anybody tries to allocate more than the buffer size we set, std::bad\_alloc would be thrown.

Let's move on to our last item in this chapter.

\hspace*{\fill} \\ %插入空行
\noindent
\textbf{清空内存}

Sometimes not having to deallocate the memory, as the monotonic buffer resource does, is still not enough for performance. A special technique called winking out can help here. Winking out objects means that they're not only not deallocated one by one, but their constructors aren't called too. The objects simply evaporate, saving time that would normally be spent calling destructors for each object and their members (and their members...) in the arena.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black, title=Note]
\hspace*{0.7cm}This is an advanced topic. Be careful when using this technique, and only use it if the possible gain is worth it.
\end{tcolorbox}

This technique can save your precious CPU cycles, but it's not always possible to use it. Avoid winking out memory if your objects handle resources other than memory. Otherwise, you will get resource leaks. The same goes if you depend on any side effects the destructors of your objects would have.

Let's now see winking out in action:

\begin{lstlisting}[style=styleCXX]
auto verbose = verbose_resource(std::pmr::get_default_resource());
auto monotonic = std::pmr::monotonic_buffer_resource(&verbose);
std::pmr::set_default_resource(&monotonic);

auto alloc = std::pmr::polymorphic_allocator{};
auto *vector = alloc.new_object<std::pmr::vector<std::pmr::string>>();
vector->push_back("first one");
vector->emplace_back("long second one that must allocate");
\end{lstlisting}

Here, we created a polymorphic allocator by hand that'll use our default resource – a monotonic one that logs each time it reaches upstream. To create objects, we'll use a C++20 addition to pmr, the new\_object function. We create a vector of strings. We can pass the first one using push\_back, because it's small enough to fit into the small-string buffer we have thanks to SSO. The second string would need to allocate a string using the default resource and only then pass it to our vector if we used push\_back. Emplacing it causes the string to be constructed inside the vector's functions (not before the call), so it will use the vector's allocator. Finally, we don't call the destructors of allocated objects anywhere, and just deallocate everything at once, when we exit the scope. This should give us hard-to-beat performance.

That was the last item on our list for this chapter. Let's summarize what we've learned.














 